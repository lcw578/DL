# 基础作业二
## 一、SOFTMAX 回归：多类别分类的概率化输出工具

SOFTMAX 回归（Softmax Regression）是一种针对多类别分类任务的经典模型，常作为神经网络的输出层函数，将模型的原始输出转化为符合概率分布的类别预测结果。它可以看作是逻辑回归（二分类）在多类别场景下的扩展。

### 1.1 核心原理：从原始输出到概率分布

在多类别分类任务中（如识别图像中的 “猫”“狗”“鸟” 等类别），模型的原始输出通常是一组实数值（称为 “logits”），每个值对应一个类别的 “分数”。SOFTMAX 函数的作用，就是将这组分数转化为**概率分布**—— 即每个类别的预测概率，且所有类别的概率之和为 1。

假设模型对某样本的原始输出为$  z_1, z_2, ..., z_K  $（K 为类别总数），则第 i 个类别的预测概率$  \hat{y}_i  $的计算方式为：

$ 
\hat{y}_i = \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
 $

从公式可以看出，SOFTMAX 函数通过指数运算放大了原始输出中的差异（数值越大的$  z_i  $，对应的概率提升越显著），同时通过分母的归一化操作，确保输出满足概率的基本性质（非负性、和为 1）。这种特性使其非常适合作为分类任务的输出层：例如在手写数字识别中，模型输出 10 个 logits（对应 0-9），SOFTMAX 将其转化为 10 个概率值，概率最高的类别即为预测结果。

### 1.2 损失函数：交叉熵与模型优化目标

为了训练 SOFTMAX 回归模型，需要定义一个损失函数来衡量预测概率与真实标签的差异。**交叉熵损失（Cross-Entropy Loss）** 是最常用的选择，其核心思想是：如果样本的真实类别为 c，则希望模型对 c 的预测概率$  \hat{y}_c  $尽可能接近 1，而其他类别的概率尽可能接近 0。

对于单个样本，设真实标签为 one-hot 向量（仅真实类别对应位置为 1，其余为 0），则交叉熵损失的计算公式为：

$ 
L = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)
 $

其中$  y_i  $为真实标签（1 或 0），$  \hat{y}_i  $为 SOFTMAX 输出的预测概率。由于真实标签中只有一个$  y_i = 1  $，损失可简化为$  L = -\log(\hat{y}_c)  $（c 为真实类别）。这意味着：当模型对真实类别的预测概率越高（越接近 1），损失越小；反之损失越大。

在训练过程中，模型通过反向传播不断调整参数，最小化交叉熵损失，最终实现对类别概率的精准预测。

### 1.3 应用场景与特点

SOFTMAX 回归的核心优势在于**概率化输出**和**多类别兼容性**，其典型应用场景包括：



*   基础多类别分类任务：如手写数字识别（MNIST）、文本主题分类等；

*   神经网络输出层：在深度神经网络中，SOFTMAX 常作为输出层激活函数，将最后一层的 logits 转化为类别概率（例如 CNN 用于图像分类时，输出层通常采用 SOFTMAX）；

*   与其他模型结合：在目标检测、语义分割等任务中，SOFTMAX 可用于对每个区域或像素的类别进行预测。

## 二、ADAM 优化器：高效稳定的参数更新策略

在神经网络训练中，优化器的作用是通过调整模型参数（如权重、偏置）来最小化损失函数。ADAM（Adaptive Moment Estimation）优化器是目前最流行的优化算法之一，它结合了动量法（Momentum）和 RMSprop 的优点，同时实现了 “动量加速” 和 “自适应学习率”，在多数场景下能快速收敛且稳定性强。

### 2.1 核心原理：动量与自适应学习率的结合

ADAM 的设计灵感来自于对 “参数更新方向” 和 “学习率大小” 的优化：



*   动量法（Momentum）：模拟物理中的 “动量” 概念，通过累积历史更新方向，减少震荡，加速收敛（例如在沟壑状损失函数中，避免来回震荡，沿梯度平缓方向快速前进）；

*   RMSprop：通过自适应调整学习率（对频繁更新的参数用较小学习率，对稀疏更新的参数用较大学习率），解决学习率 “一刀切” 的问题。

ADAM 将两者结合，通过两个核心变量实现优化：

#### （1）动量项（一阶矩估计）

记录参数更新的 “累积方向”，缓解震荡。计算公式为：

$ 
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t
 $

其中，$  m_t  $为 t 时刻的动量项，$  g_t  $为 t 时刻的梯度（损失函数对参数的偏导数），$  \beta_1  $为动量衰减系数（通常取 0.9）。可以看出，$  m_t  $是历史梯度的指数移动平均，相当于 “平滑后的梯度方向”，减少了单次梯度波动的影响。

#### （2）自适应学习率项（二阶矩估计）

记录参数更新的 “历史梯度平方的累积”，用于动态调整学习率。计算公式为：

$ 
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
 $

其中，$  v_t  $为 t 时刻的二阶矩估计，$  \beta_2  $为二阶矩衰减系数（通常取 0.999）。$  v_t  $反映了梯度的 “波动性”：若某参数的梯度一直较大（如频繁更新的权重），$  v_t  $较大，对应学习率会被缩小；若梯度稀疏（如偶尔更新的偏置），$  v_t  $较小，学习率会被放大。

#### （3）参数更新公式

由于初始时刻$  m_0 = 0  $、$  v_0 = 0  $，ADAM 会对$  m_t  $和$  v_t  $进行 “偏差修正”，消除初始值为 0 的影响：

$ 
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
 $

最终的参数更新公式为：

$ 
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t
 $

其中，$  \theta  $为模型参数，$  \alpha  $为初始学习率（通常取 0.001），$  \epsilon  $为极小值（通常取$  10^{-8}  $），用于避免分母为 0。

### 2.2 优势与适用场景

ADAM 优化器的核心优势在于：

*   **收敛速度快**：动量项加速了沿稳定方向的更新，减少震荡；

*   **稳定性强**：自适应学习率避免了单一学习率对不同参数的 “不适配” 问题；

*   **鲁棒性高**：对初始学习率的敏感度低（即使$  \alpha  $略有偏差，仍能较好收敛），适用于大规模数据和复杂网络（如深度学习中的 CNN、RNN、Transformer 等）。

其典型应用场景包括：

*   深度神经网络训练（如 ResNet、BERT 等模型的预训练）；

*   数据分布复杂或稀疏的任务（如自然语言处理、推荐系统）；

*   对收敛速度要求较高的场景（如快速原型验证、大规模数据集训练）。



## 三、AlexNet
### 3.1 诞生背景与开创性成果

在 2012 年之前，尽管深度学习已经有所发展，但在大规模图像识别任务中，传统方法依然占据主导地位。当时的技术在处理复杂多样的图像数据时，面临着诸多挑战，如特征提取困难、模型泛化能力差等问题。AlexNet 的横空出世彻底打破了这一局面。由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 共同开发的 AlexNet，在 ImageNet 大规模视觉识别挑战赛（ILSVRC）中展现出了惊人的实力。它以 15.3% 的 top - 5 错误率一举夺冠，与亚军相比，错误率大幅降低了 10.8 个百分点。这一成绩不仅让学术界和工业界看到了深度卷积神经网络处理大规模图像数据的巨大潜力，更引发了对深度学习研究和应用的热潮，成为深度学习发展历程中的一个重要转折点。

### 3.2 网络架构详解

AlexNet 的网络架构设计在当时具有创新性，主要包含以下几个关键部分：



*   **卷积层与池化层交替**：整个网络由 8 层构成，其中前 5 层为卷积层。在第一层卷积中，采用了 11×11 的较大卷积核，步长设置为 4。这种配置能够在图像上快速滑动，提取出显著的特征，同时有效减少计算量。对于输入尺寸为 227×227×3 的图像，经过第一层卷积后，输出的特征图尺寸为 55×55×96。后续的卷积层则通过逐渐减小卷积核尺寸，如 3×3 等，并不断增加通道数，进一步对特征进行细化提取。在部分卷积层之后，连接了最大池化层。最大池化层使用 3×3 的滤波器，步长为 2，且采用重叠池化的方式。这种重叠操作在降低特征图分辨率的同时，更好地保留了相邻特征之间的空间关系，实验表明，相较于非重叠池化，它降低了 top - 1 和 top - 5 错误率，有助于提升模型的性能。

*   **全连接层决策**：网络的最后 3 层是全连接层。前两个全连接层分别包含 4096 个神经元，并且在这两层中使用了 Dropout 技术，Dropout 概率设置为 0.5。Dropout 能够在训练过程中随机丢弃部分神经元，减少神经元之间的复杂共适应关系，从而有效防止模型过拟合。最后，通过 Softmax 层将全连接层输出的特征映射到类别空间，输出各类别的预测概率，完成图像分类任务。

*   **ReLU 激活函数加速训练**：AlexNet 在激活函数的选择上做出了重大改进，创新性地使用 ReLU（Rectified Linear Unit）激活函数替代传统的 tanh 和 sigmoid 函数。ReLU 函数的表达式为$f(x)=\max(0,x)$，在正区间具有线性特性。这种特性使得在训练过程中，当神经元的输入为正数时，梯度能够保持稳定，避免了梯度消失问题，从而使网络的训练速度大幅提升。实验结果显示，相比使用其他激活函数，采用 ReLU 函数的 AlexNet 训练时间缩短了约 6 倍。

### 3.3 关键技术与优势



*   **数据增强抗过拟合**：由于 AlexNet 的参数数量众多，达到了 6000 万个，在训练过程中极易出现过拟合现象。为了解决这一问题，AlexNet 采用了数据增强技术。通过对训练图像进行随机裁剪、水平翻转、颜色抖动等操作，动态地扩充了数据集的规模和多样性。这样，模型在训练时能够接触到更多不同形式的图像样本，增强了模型的泛化能力，减少了对特定样本的过拟合风险。同时，在全连接层应用的 Dropout 技术，进一步降低了神经元之间的协同适应，使得模型在面对新数据时能够表现得更加稳健。

*   **GPU 并行计算提升效率**：在当时，单块 GPU 的内存有限，无法容纳整个 AlexNet 模型进行训练。AlexNet 的研究者们充分利用两块 NVIDIA GTX 580 GPU 进行并行计算，将网络分为两部分分别在两块 GPU 上运行。这种并行计算的方式极大地加速了训练进程，使得大规模深度神经网络的训练在合理时间内成为可能。它不仅为 AlexNet 的成功训练提供了保障，也为后续基于 GPU 的深度学习加速技术的发展奠定了基础，推动了深度学习在计算资源利用方面的创新。

### 3.4 应用场景

AlexNet 的成功使其在多个计算机视觉基础任务中得到广泛应用：



*   **图像分类**：在早期的图像搜索引擎中，需要对海量的图片进行分类标注，以便用户能够快速准确地检索到所需图片。AlexNet 凭借其强大的特征提取和分类能力，能够高效地对图片进行分类，为图像搜索引擎的性能提升提供了有力支持。

*   **目标检测**：在安防监控领域，需要对监控视频中的目标物体进行实时检测和分类。AlexNet 可以对视频中的每一帧图像进行处理，初步识别出可能存在的目标物体类别，如人、车辆等，为后续的精准分析和事件预警提供基础。

## 四、ResNet
### 4.1 深度神经网络的困境与 ResNet 的解决方案

随着对神经网络研究的深入，人们发现增加网络的层数理论上可以让模型学习到更复杂的特征表示，从而提升模型的性能。然而，在实际训练过程中，却遇到了梯度消失或梯度爆炸的严重问题。当网络层数不断增加时，梯度在反向传播过程中经过多层传递后，会逐渐变得非常小（梯度消失）或非常大（梯度爆炸），这使得网络难以收敛，训练误差和测试误差不仅没有降低，反而出现上升的情况。2015 年，微软研究院的学者提出了 ResNet（Residual Network），通过引入残差模块（Residual Block）和跳连（Skip Connection）机制，巧妙地解决了这一难题，使得训练超深神经网络成为现实。

### 4.2 残差模块与跳连机制剖析

*   **残差模块核心设计**：ResNet 的核心在于残差模块的设计。在一个子网络中，设输入为$x$，原本期望网络学习到的映射为$H(x)$。而 ResNet 让网络去学习残差映射$f(x)=H(x)-x$。通过跳连结构，将输入$x$直接与子网络输出的残差$f(x)$相加，得到最终输出$H(x)=f(x)+x$。这种设计的巧妙之处在于，网络在学习时只需要关注输入与期望输出之间的差异部分（即残差），而不是直接学习复杂的$H(x)$映射。这大大降低了学习的难度，尤其是在网络深度较大时，能够让网络更容易收敛和学习到有效的特征。

*   **缓解梯度消失 / 爆炸**：在反向传播过程中，跳连机制起到了至关重要的作用。由于跳连使得梯度能够直接从深层传递到浅层，避免了梯度在多层传递中逐渐消失或爆炸的问题。这意味着即使网络层数增加到 100 甚至 1000 层，梯度依然能够有效地传播，保证了网络的稳定训练和收敛。这种机制极大地拓展了神经网络的深度，使得模型能够学习到更丰富、更高级的特征表示。

### 4.3 网络架构特点

*   **以 VGG 为基础的架构改进**：ResNet 的网络架构在一定程度上参考了 VGG - 19 的结构。它采用了 34 层的基础网络结构，在此之上引入跳连机制，将其转化为残差网络。不同深度的 ResNet，如 ResNet - 18、ResNet - 34、ResNet - 50 等，通过调整残差模块的数量来构建。这种灵活的设计方式能够根据不同的任务复杂度和数据集规模，选择合适深度的网络结构，以达到最佳的性能表现。

*   **多样化的应用拓展**：ResNet 的设计理念不仅在图像分类任务中取得了巨大成功，还被广泛应用于其他计算机视觉任务：


    *   **目标检测**：在目标检测任务中，基于 ResNet 提取的特征能够更准确地定位和识别目标物体。通过在 ResNet 的基础上添加区域提议网络（RPN）等组件，可以实现对图像中不同目标物体的检测和分类。

    *   **语义分割**：在语义分割任务中，ResNet 通过对不同尺度特征的融合，能够实现对图像中每个像素的类别标注。通过将 ResNet 与 U - Net 等分割网络结构相结合，可以有效地分割出图像中的不同物体和区域。

### 4.4 优势与应用

*   **卓越的性能表现**：ResNet 在 ImageNet 挑战赛中展现出了卓越的性能，大幅降低了错误率，超越了同期的其他模型。其强大的特征学习能力使得它能够处理复杂的图像数据，在实际应用中取得了良好的效果。例如，在医学图像分析中，ResNet 可以准确地识别出病变区域；在自动驾驶中的场景识别中，能够快速准确地识别道路、车辆、行人等目标物体。

*   **泛化能力强**：由于成功解决了深度神经网络的训练难题，ResNet 能够学习到更丰富、更具代表性的特征，模型的泛化能力得到显著提升。在不同领域的图像数据上，无论数据的分布和特征如何变化，ResNet 都能展现出较好的适应性和准确性，能够有效地应用于各种实际场景。


# 对于本次作业的认知
我对于深度学习的认知如下：深度学习的核心逻辑是通过构建神经网络结构，使模型在大规模标注数据中自主学习数据内在规律，进而训练出能够稳定解决特定任务的模型。以物体识别与分类任务为例，其关键在于让模型从图像的像素级原始信息中，逐步提取并理解不同类别物体的区分性特征 —— 例如在本次锥桶颜色三分类的识别中，模型会自主捕捉锥桶中具有类别差异的特征。

在本次三分类任务实践中，我深刻体会到模型构建与参数调试的系统性与复杂性。仅网络结构设计就需要多维度考量：卷积层的核心功能是特征提取，其层数选择需结合任务特征复杂度。若待分类对象为三种形状差异显著的几何图形，2 至 3 层卷积通常可满足特征提取需求；若需区分三种纹理特征相近的植物叶片，则需增加卷积层数至 4 层左右，甚至可借鉴 AlexNet 中 “卷积 + 池化” 的交替结构 —— 通过池化层对特征图进行下采样，在保留关键特征的同时降低计算量。全连接层的作用是将卷积层提取的高维特征映射至类别空间，一般设置 1 至 2 层即可，但神经元数量需通过实验确定：数量过少会导致特征拟合不足，过多则易引发过拟合风险。通道数配置需遵循递增原则，通常从输入图像的基础通道数（如 RGB 图像为 3 通道）开始，按 32→64→128 的梯度逐步提升，以实现特征的精细化提取，但需以硬件显存为约束 —— 若中间层通道数设置过高导致显存溢出，需重新调整网络结构。

参数设置需与数据规模及硬件性能相匹配。Batchsize 与 epoch 的确定直接依赖于数据集大小与显卡显存容量。本次实验所用数据集包含约 2000 张图像，硬件为显存 16G 的消费级显卡。经多次测试，Batchsize 设为 32 时性能最优：设为 64 时，单次迭代的显存占用接近阈值，易导致训练过程卡顿；设为 16 时，训练效率显著下降，同等 epoch 下耗时增加约 50%。Epoch 的选择需在欠拟合与过拟合之间寻求平衡：初始设置 400 个 epoch 时，模型性能仍呈上升趋势，故调整至 200 个，避免过拟合问题。

优化器与学习率的选择对模型收敛效果至关重要。理论研究表明 Adam 优化器具有较强的自适应能力，本次实验验证了其在中小规模数据集上的适用性 —— 与 SGD 相比，Adam 的收敛速度更快，对初始学习率的敏感度更低。学习率调试采用梯度递减策略：初始设置为 0.001 时，模型在训练前期性能提升显著；当模型性能进入平台期后，将学习率调整为 0.0001 进行精细优化，验证集准确率可进一步提升 1 至 2 个百分点。这一过程让我认识到，深度学习中的参数选择不存在普适性标准 —— 如 ResNet 的残差结构虽能有效解决深层网络的梯度问题，但在三分类任务中是否引入跳连、跳连层数设置等，均需通过实验验证其实际效果。

我认为神经网络的 “黑箱” 特性较为明显，其内部决策过程难以通过完整的数学模型进行精确描述 —— 特定参数组合的有效性，往往源于其与数据潜在规律的契合，而非可推导的数学逻辑。因此，参数调试对我而言不仅是试错过程，更是经验积累的过程：当模型出现过拟合迹象时，优先尝试增加 Dropout 层或减小 Batchsize；当模型性能停滞时，可更换优化器或微调学习率。在部分实验中，合理的参数调整可使模型准确率从 70% 提升至 90%，这种从性能不稳定到稳定达标的过程，是深度学习实践中极具成就感的体验。



